# Configurações padrão do Spark para o ambiente Docker

# Configuração do Catálogo Iceberg (Usando o tipo Hadoop para o Data Lake local)
spark.sql.catalog.iceberg_catalog=org.apache.spark.sql.iceberg.catalog.SparkCatalog
spark.sql.catalog.iceberg_catalog.type=hadoop
# O caminho deve ser o mesmo volume montado no docker-compose.yml
spark.sql.catalog.iceberg_catalog.warehouse=file:///opt/bitnami/spark/data_lake/iceberg_warehouse

# Configuração para logs de eventos do History Server
spark.eventLog.enabled=true
spark.eventLog.dir=file:///opt/bitnami/spark/data_lake/spark-events

# Configuração para o History Server
spark.history.fs.logDirectory=file:///opt/bitnami/spark/data_lake/spark-events

# Configuração de pacotes (JARs) - O usuário deve baixar e colocar no diretório 'jars'
# Estes pacotes serão carregados automaticamente pelo Spark no ambiente bitnami
# spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.6.1,org.postgresql:postgresql:42.7.3
# Como estamos usando o volume 'jars', o Spark carregará os JARs automaticamente.
# No entanto, é bom ter a configuração de pacotes para referência.


